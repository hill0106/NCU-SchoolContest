# Testing Guide for NCU Alumni Employment Analysis

This document provides comprehensive information about testing the NCU Alumni Employment Analysis project.

## ðŸ§ª Test Overview

The project includes a comprehensive test suite that covers:
- **Unit Tests**: Individual function testing
- **Integration Tests**: Component interaction testing
- **Data Tests**: Data integrity and validation
- **Visualization Tests**: Plot generation testing
- **Performance Tests**: Speed and memory usage
- **Edge Case Tests**: Error handling and boundary conditions

## ðŸ“ Test Structure

```
â”œâ”€â”€ test_main.py              # Main test suite
â”œâ”€â”€ test_runner.py            # Test runner script
â”œâ”€â”€ generate_test_data.py     # Test data generator
â”œâ”€â”€ requirements-test.txt     # Testing dependencies
â”œâ”€â”€ pytest.ini              # Pytest configuration
â”œâ”€â”€ .github/workflows/       # CI/CD testing
â”‚   â””â”€â”€ test.yml
â””â”€â”€ test_data/               # Generated test data
    â”œâ”€â”€ first_test.csv
    â”œâ”€â”€ third_test.csv
    â”œâ”€â”€ fifth_test.csv
    â””â”€â”€ edge_case_data/
```

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# Install testing dependencies
pip install -r requirements-test.txt

# Or use the test runner
python test_runner.py --install-deps
```

### 2. Generate Test Data

```bash
# Generate synthetic test data
python generate_test_data.py
```

### 3. Run Tests

```bash
# Run all tests
python test_runner.py --type all

# Run specific test types
python test_runner.py --type unit
python test_runner.py --type integration
python test_runner.py --type data
```

## ðŸ”§ Test Types

### Unit Tests
Test individual functions and classes in isolation.

```bash
python test_runner.py --type unit
```

**Coverage:**
- Data processing functions (`operateJobStr`, `classifyJob`)
- Class creation (`job`, `majorJob`, `college135`)
- Basic data manipulation

### Integration Tests
Test how different components work together.

```bash
python test_runner.py --type integration
```

**Coverage:**
- Data flow between functions
- File I/O operations
- Cross-module interactions

### Data Tests
Validate data integrity and structure.

```bash
python test_runner.py --type data
```

**Coverage:**
- CSV file structure validation
- Required column presence
- Data type consistency
- Missing value handling

### Visualization Tests
Test plot generation and visualization functions.

```bash
python test_runner.py --type visualization
```

**Coverage:**
- Chart generation
- File output creation
- Plot configuration

### Performance Tests
Measure execution time and memory usage.

```bash
python test_runner.py --type performance
```

**Coverage:**
- Large dataset processing
- Memory usage monitoring
- Execution time benchmarks

## ðŸ“Š Test Data

### Generated Test Data
The test suite uses synthetic data generated by `generate_test_data.py`:

- **Realistic Data**: Mirrors actual survey structure
- **Multiple Colleges**: All 9 colleges represented
- **Various Departments**: 20+ departments across colleges
- **Employment Status**: Full-time, part-time, unemployed
- **Industry Categories**: 10+ industry classifications
- **Time Periods**: 1-year, 3-year, 5-year post-graduation

### Edge Case Data
Special test cases for robust testing:

- **Empty Data**: Empty DataFrames
- **Missing Values**: NULL and empty string handling
- **Special Characters**: Unicode and formatting issues
- **Invalid Data**: Malformed entries

## ðŸ› ï¸ Test Configuration

### Pytest Configuration (`pytest.ini`)
```ini
[tool:pytest]
testpaths = .
python_files = test_*.py
addopts = --verbose --cov=main --cov-report=html
```

### Test Markers
```python
@pytest.mark.unit          # Unit tests
@pytest.mark.integration   # Integration tests
@pytest.mark.slow          # Performance tests
@pytest.mark.visualization # Plot tests
@pytest.mark.data          # Data tests
```

## ðŸ” Running Specific Tests

### Run by Test Class
```bash
python -m pytest test_main.py::TestDataProcessing -v
```

### Run by Test Method
```bash
python -m pytest test_main.py::TestDataProcessing::test_operateJobStr -v
```

### Run with Markers
```bash
python -m pytest -m "unit and not slow" -v
```

### Run with Coverage
```bash
python -m pytest --cov=main --cov-report=html
```

## ðŸ“ˆ Test Reports

### HTML Coverage Report
After running tests with coverage, view the report:
```bash
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

### Test Results XML
JUnit-compatible results for CI/CD:
```bash
python -m pytest --junitxml=test-results.xml
```

### Performance Reports
```bash
python -m pytest --benchmark-only
```

## ðŸš¦ Continuous Integration

### GitHub Actions
Automated testing on every push and pull request:

- **Multiple Python Versions**: 3.8, 3.9, 3.10, 3.11
- **Code Quality**: Linting, formatting checks
- **Security Scanning**: Bandit, Safety
- **Coverage Reporting**: Codecov integration

### Local CI Simulation
```bash
# Run all CI checks locally
python test_runner.py --type lint
python test_runner.py --type format
python test_runner.py --type all
```

## ðŸ› Debugging Tests

### Verbose Output
```bash
python -m pytest -v -s
```

### Debug Mode
```bash
python -m pytest --pdb
```

### Stop on First Failure
```bash
python -m pytest -x
```

### Show Local Variables
```bash
python -m pytest -l
```

## ðŸ“ Writing New Tests

### Test Function Structure
```python
def test_function_name(self):
    """Test description"""
    # Arrange
    input_data = create_test_data()
    
    # Act
    result = function_under_test(input_data)
    
    # Assert
    self.assertEqual(result, expected_output)
```

### Test Class Structure
```python
class TestNewFeature(unittest.TestCase):
    """Test new feature functionality"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.test_data = create_fixture()
    
    def test_feature_behavior(self):
        """Test specific behavior"""
        # Test implementation
        pass
```

### Mocking External Dependencies
```python
@patch('main.external_function')
def test_with_mock(self, mock_external):
    """Test with mocked external dependency"""
    mock_external.return_value = expected_value
    result = function_under_test()
    self.assertEqual(result, expected_result)
```

## ðŸ”§ Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Ensure main.py is in the same directory
   export PYTHONPATH=.
   ```

2. **Missing Dependencies**
   ```bash
   pip install -r requirements-test.txt
   ```

3. **Data File Issues**
   ```bash
   # Generate test data
   python generate_test_data.py
   ```

4. **Permission Errors**
   ```bash
   # Make test runner executable
   chmod +x test_runner.py
   ```

### Test Environment Setup
```bash
# Create virtual environment
python -m venv test_env
source test_env/bin/activate  # Linux/macOS
# or
test_env\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements-test.txt

# Run tests
python test_runner.py --type all
```

## ðŸ“Š Test Metrics

### Coverage Goals
- **Line Coverage**: > 80%
- **Branch Coverage**: > 70%
- **Function Coverage**: > 90%

### Performance Benchmarks
- **Data Processing**: < 5 seconds for 1000 records
- **Visualization**: < 10 seconds for complex plots
- **Memory Usage**: < 500MB for large datasets

## ðŸ¤ Contributing

### Adding New Tests
1. Create test functions following naming convention
2. Add appropriate test markers
3. Include docstrings explaining test purpose
4. Update this documentation if needed

### Test Review Checklist
- [ ] Tests cover happy path scenarios
- [ ] Edge cases are handled
- [ ] Error conditions are tested
- [ ] Tests are independent and isolated
- [ ] Mocking is used appropriately
- [ ] Assertions are specific and meaningful

## ðŸ“š Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Unittest Documentation](https://docs.python.org/3/library/unittest.html)
- [Coverage.py Documentation](https://coverage.readthedocs.io/)
- [Mock Documentation](https://docs.python.org/3/library/unittest.mock.html)

---

**Note**: This testing framework is designed to be comprehensive yet maintainable. Regular test execution ensures code quality and prevents regressions.
